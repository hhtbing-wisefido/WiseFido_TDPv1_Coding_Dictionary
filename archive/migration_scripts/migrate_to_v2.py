"""
æ•°æ®è¿ç§»è„šæœ¬ï¼šv1.x â†’ v2.0.0
ç§»é™¤å­—æ®µï¼šid, category, status, version, description, description_zh, synonyms, synonyms_zh, 
          source_refs, detection, fhir
ä¿ç•™å­—æ®µï¼šsystem, code, display, display_zh
"""
import json
from pathlib import Path
from datetime import datetime

def migrate_coding_entry(old_entry):
    """è¿ç§»å•ä¸ªè¯æ¡"""
    return {
        "system": old_entry["system"],
        "code": old_entry["code"],
        "display": old_entry["display"],
        "display_zh": old_entry["display_zh"]
    }

def archive_removed_fields(old_entry, archive_dir):
    """å½’æ¡£è¢«ç§»é™¤çš„å­—æ®µ"""
    # ä½¿ç”¨ system|code ç»„åˆä½œä¸ºæ–‡ä»¶åï¼ˆé¿å… Windows æ–‡ä»¶åéæ³•å­—ç¬¦ï¼‰
    system = old_entry.get("system", "unknown")
    code = old_entry["code"]
    # æ›¿æ¢æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
    safe_filename = f"{system}_{code}".replace("://", "_").replace("/", "_").replace(":", "_")
    
    removed_data = {}
    
    fields_to_archive = [
        "id", "category", "status", "version", 
        "description", "description_zh",
        "synonyms", "synonyms_zh",
        "source_refs", "detection", "fhir"
    ]
    
    for field in fields_to_archive:
        if field in old_entry:
            removed_data[field] = old_entry[field]
    
    if removed_data:
        archive_file = archive_dir / f"{safe_filename}.json"
        with open(archive_file, 'w', encoding='utf-8') as f:
            json.dump(removed_data, f, ensure_ascii=False, indent=2)
        return True
    return False

def main():
    print("=== å¼€å§‹æ•°æ®è¿ç§» v1.x â†’ v2.0.0 ===\n")
    
    # æ–‡ä»¶è·¯å¾„
    dict_file = Path("coding_dictionary/coding_dictionary.json")
    archive_dir = Path("archive/removed_fields_v1.2.6")
    backup_file = Path("temp/backups/coding_dictionary.json.v1.backup")
    
    # åˆ›å»ºå½’æ¡£ç›®å½•
    archive_dir.mkdir(parents=True, exist_ok=True)
    
    # è¯»å–åŸå§‹æ•°æ®
    print(f"ğŸ“– è¯»å–åŸå§‹æ•°æ®: {dict_file}")
    with open(dict_file, 'r', encoding='utf-8') as f:
        old_data = json.load(f)
    
    old_codings = old_data if isinstance(old_data, list) else old_data.get("codings", [])
    print(f"   åŸè¯æ¡æ•°: {len(old_codings)}")
    
    # å¤‡ä»½åŸæ–‡ä»¶
    print(f"\nğŸ’¾ å¤‡ä»½åŸæ–‡ä»¶: {backup_file}")
    with open(backup_file, 'w', encoding='utf-8') as f:
        json.dump(old_data, f, ensure_ascii=False, indent=2)
    
    # è¿ç§»æ•°æ®
    print(f"\nğŸ”„ å¼€å§‹è¿ç§»...")
    new_codings = []
    archived_count = 0
    
    for idx, old_entry in enumerate(old_codings, 1):
        # å½’æ¡£è¢«ç§»é™¤çš„å­—æ®µ
        if archive_removed_fields(old_entry, archive_dir):
            archived_count += 1
        
        # è¿ç§»åˆ°æ–°ç»“æ„
        new_entry = migrate_coding_entry(old_entry)
        new_codings.append(new_entry)
        
        # æ˜¾ç¤ºè¿›åº¦
        if idx % 10 == 0 or idx == len(old_codings):
            print(f"   è¿›åº¦: {idx}/{len(old_codings)}")
    
    # å†™å…¥æ–°æ•°æ®ï¼ˆä¿æŒåŸæœ‰çš„åˆ—è¡¨æ ¼å¼ï¼‰
    print(f"\nğŸ’¾ å†™å…¥æ–°æ•°æ®: {dict_file}")
    with open(dict_file, 'w', encoding='utf-8') as f:
        json.dump(new_codings, f, ensure_ascii=False, indent=2)
    
    # ç»Ÿè®¡
    print(f"\nâœ… è¿ç§»å®Œæˆï¼")
    print(f"   æ–°è¯æ¡æ•°: {len(new_codings)}")
    print(f"   å½’æ¡£è¯æ¡æ•°: {archived_count}")
    print(f"   å½’æ¡£ç›®å½•: {archive_dir}")
    print(f"   å¤‡ä»½æ–‡ä»¶: {backup_file}")

if __name__ == "__main__":
    main()
